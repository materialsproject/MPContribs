{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas>=2.3.0 pyarrow>=21.0.0 boto3>=1.40.0 python-dotenv>=1.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Making a large-data download from OpenData\n",
    "\n",
    "To upload larger data objects, such as raw data, to MPContribs, you will need to obtain IAM credentials from MP staff. You should also think carefully about how you structure your data so that it is amenable to cloud storage.\n",
    "Formats like `parquet` for columnar data and `zarr` for hierarchical data will aid in partial retrieval of data.\n",
    "`parquet` also permits easily filtering data.\n",
    "\n",
    "In this example, we will simulate using a `.env` file to store AWS credentials:\n",
    "```bash\n",
    ">>> ~/.env\n",
    "aws_access_key_id=abcdefg...\n",
    "aws_secret_access_key=abcdefg...\n",
    "```\n",
    "\n",
    "And convert the JSON data file to a single parquet file for download:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "\n",
    "OPENDATA_BUCKET = \"materialsproject-contribs\"\n",
    "PROJECT_NAME = \"test_solid_data\"\n",
    "\n",
    "with gzip.open(\"cubic_solid_expt_data.json.gz\", \"rt\") as f:\n",
    "    user_data = json.load(f)\n",
    "\n",
    "data = pd.DataFrame(user_data[\"data\"])\n",
    "arrow_table = pa.Table.from_pandas(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "As a reasonable best practice, we will embed the citation and column metadata in the `arrow` table schema. To do this, the metadata must be a dict of string keys mapped to strings (or bytes in both cases). This means we must flatten the `metadata` dict of the dataset, and serialize its values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "pq.write_table(\n",
    "    arrow_table.replace_schema_metadata(\n",
    "        metadata={\n",
    "            **arrow_table.schema.metadata,\n",
    "            **{\n",
    "                f\"{key}.{sub_key}\": json.dumps(v)\n",
    "                for key, vals in user_data[\"metadata\"].items()\n",
    "                for sub_key, v in vals.items()\n",
    "            },\n",
    "        }\n",
    "    ),\n",
    "    \"solid_data.parquet\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "The following convenience functions are designed to make it easy to upload or delete data from AWS S3 OpenData."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "\n",
    "def get_s3_client():\n",
    "    \"\"\"Start an S3 client from credentials stored in a ~/.env file.\"\"\"\n",
    "    load_dotenv()\n",
    "    return boto3.client(\n",
    "        \"s3\",\n",
    "        **{k: os.environ[k] for k in (\"aws_access_key_id\", \"aws_secret_access_key\")},\n",
    "    )\n",
    "\n",
    "\n",
    "def upload_single_file_to_aws(\n",
    "    in_path: str | Path,\n",
    "    out_path: str | Path | None = None,\n",
    ") -> None:\n",
    "    \"\"\"Convenience function to upload a file to S3.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    in_path : str | Path\n",
    "        The path to the file to be uploaded.\n",
    "    out_path : str, Path, or None\n",
    "        If a str or Path, the prefix and key of the file to upload.\n",
    "        If None (default), defaults to `s3://materialsproject-contribs/{PROJECT_NAME}/{file_name relative to cwd()}`.\n",
    "    \"\"\"\n",
    "    out_path = str(out_path or Path(PROJECT_NAME) / str(Path(in_path).name))\n",
    "    with open(str(in_path), \"rb\") as f:\n",
    "        get_s3_client().upload_fileobj(f, Bucket=OPENDATA_BUCKET, Key=out_path)\n",
    "\n",
    "\n",
    "def delete_single_file_in_aws(in_path: str) -> None:\n",
    "    \"\"\"Remove a key from your OpenData bucket, specified by `in_path`.\"\"\"\n",
    "    get_s3_client().delete_object(\n",
    "        Bucket=OPENDATA_BUCKET, Key=str(Path(PROJECT_NAME) / str(Path(in_path).name))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Now we simply upload the parquet file we created earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_single_file_to_aws(\"solid_data.parquet\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
